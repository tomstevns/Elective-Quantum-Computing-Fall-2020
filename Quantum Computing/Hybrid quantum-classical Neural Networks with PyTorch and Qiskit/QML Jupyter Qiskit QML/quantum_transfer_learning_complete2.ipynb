{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantum_transfer_learning_complete2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne_0JxBXWjxf",
        "outputId": "f6c1b048-4961-481b-f548-e32524a24bd3"
      },
      "source": [
        "!pip install pennylane"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pennylane\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/7b/ebe11ec7ec45882cee379ea2e6026a1452205575b6aca3bd61a8b256f373/PennyLane-0.12.0-py3-none-any.whl (401kB)\n",
            "\r\u001b[K     |▉                               | 10kB 13.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 11.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40kB 6.9MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 102kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 133kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 143kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 153kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 174kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 184kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 194kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 204kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 215kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 235kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 245kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 256kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 266kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 276kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 286kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 296kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 307kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 317kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 327kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 337kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 348kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 358kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 368kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 378kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 389kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 399kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pennylane) (1.4.1)\n",
            "Collecting semantic-version==2.6\n",
            "  Downloading https://files.pythonhosted.org/packages/28/be/3a7241d731ba89063780279a5433f5971c1cf41735b64a9f874b7c3ff995/semantic_version-2.6.0-py3-none-any.whl\n",
            "Collecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from pennylane) (2.5)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.6/dist-packages (from pennylane) (1.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pennylane) (1.18.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->pennylane) (4.4.2)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd->pennylane) (0.16.0)\n",
            "Installing collected packages: semantic-version, appdirs, pennylane\n",
            "Successfully installed appdirs-1.4.4 pennylane-0.12.0 semantic-version-2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmnRcemWav-"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Pennylane\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# OpenMP: number of parallel threads.\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibJQA4rCWpkm"
      },
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "junVoVCBWr9z",
        "outputId": "244251e4-0fa7-4b99-92f7-c84c0532a91f"
      },
      "source": [
        "data_path = r\"/content/drive/My Drive/cifar10/data\"\n",
        "\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) # mean and std from resnet18\n",
        "    ]))\n",
        "\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "    ]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fomYY24hWspj"
      },
      "source": [
        "import random\n",
        "class_names = ['airplane', 'bird']\n",
        "label_map = {0: 0, 2: 1}\n",
        "\n",
        "cifar2_train = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]\n",
        "              \n",
        "nt = len(cifar2_train) \n",
        "nv = len(cifar2_val)\n",
        "n_train = int(400)  \n",
        "n_val = int(100)\n",
        "idxT = list(range(nt)) \n",
        "idxV = list(range(nv))  \n",
        "random.shuffle(idxT)  \n",
        "random.shuffle(idxV)\n",
        "\n",
        "train_idx = idxT[:n_train]\n",
        "val_idx = idxV[:n_val]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLb07q83WvCP",
        "outputId": "6fdb93c8-3e8c-4013-c8ff-7c445842156c"
      },
      "source": [
        "from torch.utils import data\n",
        "image_datasets_raw = {'train': cifar2_train, 'val': cifar2_val}\n",
        "image_datasets = {x: data.Subset(image_datasets_raw[x], train_idx if x == 'train' else val_idx) for x in image_datasets_raw }\n",
        "\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=2) for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "print(f\"Training on device {device}.\")\n",
        "use_cuda = torch.cuda.is_available()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cpu.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T85YkLhW5dU"
      },
      "source": [
        "def train_model(model, loss_fn, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train() \n",
        "            else:\n",
        "                model.eval()  \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "               #forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # inp, dim\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "\n",
        "                    # backwards pass\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() # calculate gradiens\n",
        "                        optimizer.step() # update params\n",
        "\n",
        "                #stats\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load weights of best model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMuV25a2bBA7"
      },
      "source": [
        "n_qubits = 4                # Number of qubits\n",
        "step = 0.0004               # Learning rate\n",
        "batch_size = 4              # Number of samples for each training step\n",
        "num_epochs = 30              # Number of training epochs\n",
        "q_depth = 6                 # Depth of the quantum circuit (number of variational layers)\n",
        "gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 7 epochs.\n",
        "q_delta = 0.01              # Initial spread of random quantum weights\n",
        "rng_seed = 3                # Seed for random number generator\n",
        "start_time = time.time()    # Start of the computation timer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBuc1D09azjW"
      },
      "source": [
        "dev = qml.device(\"default.qubit\", n_qubits) # name of device, number of qubits\n",
        "#Any computational object that can apply quantum operations, and return an measurement value is called a quantum device.\n",
        "# in this example i use the default.qubit device, which is a pure qubit state device"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0g_teflXHGl"
      },
      "source": [
        "# quantumn functions\n",
        "def H_layer(nqubits): # superposition func\n",
        "    for idx in range(nqubits): # loop for each qubit in system\n",
        "        qml.Hadamard(wires=idx) # apply the h-gate on this qubit\n",
        "\n",
        "\n",
        "def RY_layer(w):# parametirized qubit rotations around the y axis\n",
        "    for idx, element in enumerate(w):\n",
        "        qml.RY(element, wires=idx)\n",
        "\n",
        "#layer for entanglement\n",
        "def entangling_layer(nqubits):\n",
        "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
        "        qml.CNOT(wires=[i, i + 1])\n",
        "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
        "        qml.CNOT(wires=[i, i + 1])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyZcLjLhc73_",
        "outputId": "22d888da-8d2f-46c2-b082-a6ed0d70f98a"
      },
      "source": [
        "qbits= 4\n",
        "for i in range(0, qbits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
        "        print(f'qubit {i} and tarbit {i+1}')\n",
        "for i in range(1, qbits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
        "        print(f'cnbit {i} and tarbit {i+1}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "qubit 0 and tarbit 1\n",
            "qubit 2 and tarbit 3\n",
            "cnbit 1 and tarbit 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPomsPxLc9fB"
      },
      "source": [
        "q0-q1-q2-q3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVBIjtBNdbtg"
      },
      "source": [
        "#quantum node \n",
        "@qml.qnode(dev, interface=\"torch\")  # quantum device, pytorch interface\n",
        "def quantum_net(q_input_features, q_weights_flat): # out_tensor,  weight\n",
        "\n",
        "    # Reshape weights\n",
        "    q_weights = q_weights_flat.reshape(q_depth, n_qubits) # Returns a tensor with the same data and number of elements as input, but with the specified shape(6 rows 4 columns)\n",
        "\n",
        "    H_layer(n_qubits) #superposition\n",
        "\n",
        "    # Embed features in the quantum node\n",
        "    RY_layer(q_input_features) # rotation around y-axis based on q_input_features\n",
        "\n",
        "    # Sequence of trainable variational layers\n",
        "    for k in range(q_depth):\n",
        "        entangling_layer(n_qubits)\n",
        "        RY_layer(q_weights[k])\n",
        "\n",
        "    # Expectation values in the Z basis\n",
        "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)] # measuring with the Z operator produces a classical output vector, suitable for additional post-processing.\n",
        "    return tuple(exp_vals) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_l6Knc6XSAn"
      },
      "source": [
        "class DressedQuantumNet(nn.Module): # subclass of the nn.Module which is the pytorch network module\n",
        "    \"\"\"\n",
        "    Torch module implementing the *dressed* quantum net.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Definition of the *dressed* layout.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.pre_net = nn.Linear(512, n_qubits) # input 512, out 4\n",
        "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits)) # 0.01 * torch.rand(6*4) # add to module parameters\n",
        "        self.post_net = nn.Linear(n_qubits, 2) # last layer\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        \"\"\"\n",
        "        Defining how tensors are supposed to move through the *dressed* quantum\n",
        "        net.\n",
        "        \"\"\"\n",
        "\n",
        "        # obtain the input features for the quantum circuit\n",
        "        # by reducing the feature dimension from 512 to 4\n",
        "        pre_out = self.pre_net(input_features) # look in init\n",
        "        q_in = torch.tanh(pre_out) * np.pi / 2.0 #activation function. with constant scaling by np.pi/2.0\n",
        "\n",
        "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
        "        q_out = torch.Tensor(0, n_qubits) # tensor of size [0,4]\n",
        "        q_out = q_out.to(device) # sends q_out to device (cpu or gpu)\n",
        "        for elem in q_in: # 4 elements\n",
        "            q_out_elem = quantum_net(elem, self.q_params).float().unsqueeze(0) # \n",
        "            q_out = torch.cat((q_out, q_out_elem)) # concatinates and therby fills the empty tensor q_out in\n",
        "\n",
        "        # return the two-dimensional prediction from the postprocessing layer\n",
        "        return self.post_net(q_out) #returns the output of the last layer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22zotevYXYxS"
      },
      "source": [
        "model_hybrid = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "for param in model_hybrid.parameters(): # sets paramters of model to not be updated by grad\n",
        "    param.requires_grad = False \n",
        "\n",
        "\n",
        "# sets last layer of the model to be the quantum layer\n",
        "model_hybrid.fc = DressedQuantumNet()\n",
        "\n",
        "# send the model to cuda or cpu according to the \"device\" object.\n",
        "model_hybrid = model_hybrid.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # loss function that combines negative likelihood loss and softmax\n",
        "\n",
        "optimizer_hybrid = optim.Adam(model_hybrid.fc.parameters(), lr=step) #step = 0.0004  \n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR( # each 7 steps decay lr by a factor of 0.1\n",
        "    optimizer_hybrid, step_size=7, gamma=gamma_lr_scheduler\n",
        ")\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx4cVYwzXh5Q",
        "outputId": "a0805aa2-9bfd-4bcb-9858-430fb5db03ca"
      },
      "source": [
        "model_hybrid = train_model(\n",
        "    model_hybrid, criterion, optimizer_hybrid, exp_lr_scheduler, num_epochs=30\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/29\n",
            "----------\n",
            "train Loss: 0.6613 Acc: 0.6075\n",
            "val Loss: 0.5748 Acc: 0.8500\n",
            "\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "train Loss: 0.5795 Acc: 0.7125\n",
            "val Loss: 0.4724 Acc: 0.9100\n",
            "\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "train Loss: 0.5646 Acc: 0.7225\n",
            "val Loss: 0.4661 Acc: 0.8900\n",
            "\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "train Loss: 0.5324 Acc: 0.7550\n",
            "val Loss: 0.3987 Acc: 0.9100\n",
            "\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "train Loss: 0.5189 Acc: 0.7450\n",
            "val Loss: 0.4331 Acc: 0.8700\n",
            "\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "train Loss: 0.5189 Acc: 0.7575\n",
            "val Loss: 0.4265 Acc: 0.8600\n",
            "\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "train Loss: 0.5206 Acc: 0.7750\n",
            "val Loss: 0.3378 Acc: 0.9100\n",
            "\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "train Loss: 0.4924 Acc: 0.7825\n",
            "val Loss: 0.3431 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "train Loss: 0.4687 Acc: 0.7900\n",
            "val Loss: 0.3346 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "train Loss: 0.4652 Acc: 0.8075\n",
            "val Loss: 0.3600 Acc: 0.8900\n",
            "\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "train Loss: 0.4453 Acc: 0.8200\n",
            "val Loss: 0.3361 Acc: 0.9100\n",
            "\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "train Loss: 0.4707 Acc: 0.8100\n",
            "val Loss: 0.3356 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "train Loss: 0.4522 Acc: 0.7975\n",
            "val Loss: 0.3386 Acc: 0.9400\n",
            "\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "train Loss: 0.4887 Acc: 0.7825\n",
            "val Loss: 0.3517 Acc: 0.9200\n",
            "\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "train Loss: 0.4780 Acc: 0.7825\n",
            "val Loss: 0.3290 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "train Loss: 0.4647 Acc: 0.7950\n",
            "val Loss: 0.3337 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "train Loss: 0.4890 Acc: 0.7625\n",
            "val Loss: 0.3397 Acc: 0.9400\n",
            "\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "train Loss: 0.4546 Acc: 0.7975\n",
            "val Loss: 0.3457 Acc: 0.9100\n",
            "\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "train Loss: 0.4667 Acc: 0.7975\n",
            "val Loss: 0.3296 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "train Loss: 0.4296 Acc: 0.8200\n",
            "val Loss: 0.3337 Acc: 0.9400\n",
            "\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "train Loss: 0.4849 Acc: 0.7800\n",
            "val Loss: 0.3305 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "train Loss: 0.4699 Acc: 0.7875\n",
            "val Loss: 0.3177 Acc: 0.9500\n",
            "\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "train Loss: 0.4874 Acc: 0.7775\n",
            "val Loss: 0.3387 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "train Loss: 0.4687 Acc: 0.7800\n",
            "val Loss: 0.3389 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "train Loss: 0.4694 Acc: 0.7850\n",
            "val Loss: 0.3314 Acc: 0.9400\n",
            "\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "train Loss: 0.4786 Acc: 0.7850\n",
            "val Loss: 0.3333 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "train Loss: 0.4696 Acc: 0.7975\n",
            "val Loss: 0.3288 Acc: 0.9300\n",
            "\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "train Loss: 0.4652 Acc: 0.7775\n",
            "val Loss: 0.3277 Acc: 0.9500\n",
            "\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "train Loss: 0.4497 Acc: 0.8075\n",
            "val Loss: 0.3356 Acc: 0.9200\n",
            "\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "train Loss: 0.4809 Acc: 0.7850\n",
            "val Loss: 0.3553 Acc: 0.9100\n",
            "\n",
            "\n",
            "Training complete in 98m 48s\n",
            "Best val Acc: 0.950000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i8sSIFpYBdd"
      },
      "source": [
        "#accuracy after 30 epochs 90% on vapl data."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}